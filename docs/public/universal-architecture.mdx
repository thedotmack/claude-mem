---
title: "Universal AI Memory Architecture"
description: "How claude-mem's architecture applies to any AI application with long-running transcripts"
---

# Universal AI Memory Architecture

**Claude-mem solves a fundamental problem in AI: context accumulation over time.**

While built for Claude Code, the architecture patterns discovered here apply to **any AI application** where context grows across sessions. This document explores how to apply these patterns beyond coding assistants.

---

## The Core Problem

Every long-running AI interaction faces the same challenge:

```
Session 1: 10,000 tokens of context
Session 2: 20,000 tokens (Session 1 + new work)
Session 3: 40,000 tokens (Sessions 1-2 + new work)
Session N: Context window exhausted ❌
```

**The universal pattern**: Any AI that accumulates episodic memory over time needs compression and retrieval.

---

## Key Features That Make It Work

### 1. **80% Token Compression**
AI-powered summarization reduces transcript size by 80% while preserving semantic meaning.

**How it works**:
```typescript
Original Tool Output (5,000 tokens):
[Full file contents, verbose logs, raw data...]

Compressed Observation (1,000 tokens):
"Discovered authentication bug in login.ts:47 - JWT validation
fails when token expires mid-request. Fixed by adding token
refresh logic before validation."
```

**Economic impact**: 80% token reduction = 80% cost reduction at scale.

### 2. **Dual-Database Architecture**
Vector databases and SQL databases solve complementary problems:

| Database Type | Strength | Use Case |
|--------------|----------|----------|
| **Vector (ChromaDB, Pinecone)** | Semantic similarity ranking | "Find similar concepts" |
| **SQL (SQLite, PostgreSQL)** | Temporal ordering & filtering | "What happened when?" |

**Why both are necessary**:
- Vector databases rank by similarity, not time (fundamental limitation)
- SQL databases rank by time efficiently but can't understand semantic meaning
- Together they enable: "Find relevant memories from the past month"

### 3. **Episodic Memory vs RAG**
Claude-mem captures the AI's own actions and learnings, not external knowledge:

| System Type | Accesses | Example |
|------------|----------|---------|
| **RAG** | External static knowledge | "Search company documentation" |
| **Episodic Memory** | AI's own history | "What bug did we fix last week?" |
| **Claude-Mem** | Both + Real-time compression | "Why did I choose this approach before?" |

**Key insight**: This is more sophisticated than pure RAG—it's autobiographical AI memory.

### 4. **Real-Time Transcript Transformation**
Compress context on-the-fly to enable indefinite session lengths:

```
Tool Execution → AI Compression (background) → Replace output → Continue session
   (5,000 tokens)    (90s timeout)              (1,000 tokens)    (within limits)
```

**Endless Mode**: Sessions can run indefinitely by compressing outputs before they return to the main thread.

### 5. **Progressive Disclosure Retrieval**
Retrieve context efficiently using a 4-step workflow:

```
1. Vector search → Top 5 candidates (index format: ~50 tokens each)
2. Review titles → Identify 2 relevant items
3. Full retrieval → Get complete details (~500 tokens each)
4. Token savings → 10x compared to loading all upfront
```

**Result**: Users get exactly the context they need without burning tokens.

---

## Application Domains

### 1. Customer Support AI Systems

**Problem**: Support agents handle 50-100 customer interactions daily, losing context each session.

**Solution**: Compress customer interaction history, maintain relationship memory across months.

**Implementation patterns**:
- Track customer journey across multiple support tickets
- Remember past issues, preferences, communication style
- Build context about product usage patterns
- Detect recurring issues that need escalation

**Example**:
```typescript
// Original transcript: 15,000 tokens
Customer: "My payment failed again"
Agent: [checks logs, reviews history, investigates...]

// Compressed observation: 500 tokens
"Customer ID 12345 experiencing recurring payment failures with
Stripe integration. Previous tickets #789, #654 showed same error.
Root cause: Bank blocking international transactions. Recommended
customer contact bank to whitelist merchant."
```

**Economic impact**: Enable 80% reduction in context re-loading costs per interaction.

### 2. Legal AI Document Review

**Problem**: Contract review spans weeks with thousands of clauses across multiple sessions.

**Solution**: Compress legal analysis, maintain understanding of evolving positions.

**Implementation patterns**:
- Track evolving legal interpretations across review sessions
- Remember precedents identified, risks flagged, arguments developed
- Build cumulative understanding of contract negotiation positions
- Maintain attorney work product across days/weeks

**Technical requirements**:
- **Dual-database**: SQL for temporal ordering of contract versions, vector for finding similar clauses
- **Compression ratio**: 85-90% (legal language is highly compressible)
- **Retrieval pattern**: Progressive disclosure with citation tracking

**Economic impact**: Enable month-long legal reviews without context loss.

### 3. Scientific Research AI Assistants

**Problem**: Literature reviews accumulate hundreds of papers over weeks/months.

**Solution**: Compress paper summaries, maintain evolving research narrative.

**Implementation patterns**:
- Track papers read, methodologies analyzed, contradictions discovered
- Build evolving hypothesis across multiple research sessions
- Remember experimental designs considered and rejected
- Maintain bibliography with annotations across long research cycles

**Example architecture**:
```typescript
// Research session structure
interface ResearchObservation {
  paper_id: string;
  key_findings: string[];
  methodology: string;
  contradictions_with: string[]; // Other paper IDs
  supports_hypothesis: boolean;
  citation_count: number;
  relevance_score: number;
}

// Dual-database usage
Vector search: "Find papers about CRISPR gene editing"
SQL filter: "Published after 2020, cited >100 times"
```

**Economic impact**: Enable PhD-level research assistance over semesters.

### 4. Content Creation Platforms

**Problem**: Creative projects evolve over weeks with character arcs, plot threads, style decisions.

**Solution**: Compress creative decisions, maintain narrative/artistic coherence.

**Use cases**:
- **Novel writing**: Track character development, plot threads, world-building decisions
- **Video production**: Remember shot selections, editing choices, director's vision
- **Art direction**: Maintain style guides, color palette decisions, design rationale

**Implementation pattern**:
```typescript
// Creative decision compression
Original session (12,000 tokens):
- Character backstory brainstorming
- Plot outline drafting
- World-building notes
- Style references

Compressed (2,000 tokens):
"Protagonist Emma changed from lawyer to teacher to better
connect with YA audience. Victorian setting moved to 1920s
for more freedom in female character agency. Magic system
simplified from 7 types to 3 after realizing complexity
detracted from character focus."
```

**Economic impact**: Support professional creators with month-long project memory.

### 5. Financial Planning & Investment AI

**Problem**: Investment strategies evolve over years with market changes and personal goals.

**Solution**: Compress financial decisions, maintain long-term strategy context.

**Implementation patterns**:
- Track investment rationale, risk tolerance changes, goal evolution
- Remember market analysis, portfolio decisions, lessons learned
- Build long-term relationship understanding financial psychology
- Maintain multi-year financial planning context

**Technical requirements**:
- **Temporal ordering critical**: Financial decisions must be chronologically accurate
- **SQL > Vector priority**: Time-series analysis is more important than semantic search
- **Compliance**: Full audit trail with original transcripts preserved

**Economic impact**: Enable AI wealth advisors with client relationships spanning decades.

### 6. Healthcare Diagnostics & Treatment Planning

**Problem**: Chronic disease management spans years with evolving symptoms and treatments.

**Solution**: Compress patient history, maintain treatment narrative.

**Implementation patterns**:
- Track symptom patterns, medication responses, lifestyle factors
- Remember treatment attempts, side effects, patient preferences
- Build long-term understanding of disease progression
- Maintain physician-patient relationship continuity

**Example**:
```typescript
// Patient session compression
Original consultation transcript: 8,000 tokens
[Symptoms, vitals, lab results, conversation...]

Compressed observation: 1,200 tokens
"Patient reports improved sleep (7hrs vs 5hrs) after melatonin.
Blood pressure normalized (125/80 from 145/95). Still experiencing
afternoon fatigue - may indicate insulin resistance. Order HbA1c
test. Consider reducing beta blocker dosage if BP stays stable."
```

**Regulatory requirements**:
- HIPAA compliance for data storage
- Full transcript preservation for legal protection
- Audit trail for all AI-generated observations
- Easy to validate against original records

**Economic impact**: Enable AI co-pilots for longitudinal patient care.

### 7. Sales & Business Development AI

**Problem**: Deal cycles span months with multiple stakeholders and evolving requirements.

**Solution**: Compress deal history, maintain relationship context.

**Implementation patterns**:
- Track stakeholder preferences, objections raised, proposals sent
- Remember competitive landscape, pricing discussions, contract negotiations
- Build cumulative understanding of customer organization dynamics
- Maintain deal strategy across multi-month sales cycles

**Economic impact**: Enable AI SDRs/AEs with relationship memory, increasing close rates.

### 8. Language Learning Tutors

**Problem**: Language acquisition happens over years with evolving proficiency and learning patterns.

**Solution**: Compress learning progress, maintain student profile.

**Implementation patterns**:
- Track vocabulary learned, grammar concepts mastered, common mistakes
- Remember learning style, motivation patterns, cultural interests
- Build long-term understanding of proficiency evolution
- Adapt teaching strategy based on accumulated student behavior

**Economic impact**: Enable personalized tutoring relationships spanning years at scale.

---

## Technical Implementation Patterns

### Pattern 1: Hook-Based Integration

**When to use**: Any application with a transcript/conversation structure.

```
┌─────────────────────────────────────────────────────────────┐
│ Input → [Pre-Hook: Inject Context] → AI Processing          │
│         → [Post-Hook: Compress & Save] → Output              │
└─────────────────────────────────────────────────────────────┘
```

**Components**:
- **Pre-hook**: Fetch relevant observations from dual-database
- **Post-hook**: Compress tool outputs/responses, save to episodic memory
- **Applicability**: Chat apps, IDEs, customer support platforms, creative tools

**Code example**:
```typescript
// Pre-hook: SessionStart
async function injectContext(sessionId: string): Promise<string> {
  // Vector search for relevant past work
  const vectorResults = await chromaDB.search({
    query: getCurrentProjectContext(),
    limit: 5
  });

  // SQL filter for recent sessions
  const recentSessions = await sqlite.query(
    'SELECT * FROM observations WHERE date > ? ORDER BY date DESC LIMIT 10',
    [Date.now() - 90 * 24 * 60 * 60 * 1000] // 90 days
  );

  // Combine and format for injection
  return formatContextForInjection(vectorResults, recentSessions);
}

// Post-hook: PostToolUse
async function compressAndSave(toolOutput: string, metadata: object) {
  // Send to worker for AI compression (non-blocking)
  await workerQueue.enqueue({
    type: 'compress_observation',
    data: { toolOutput, metadata },
    timeout: 90000 // 90s max
  });
}
```

### Pattern 2: Async Worker Architecture

**When to use**: Applications with strict timeout requirements.

```
┌─────────────────────────────────────────────────────────────┐
│ Main Thread (hooks) → Message Queue → Background Worker     │
│                                        (AI compression)       │
│                                        → Database            │
└─────────────────────────────────────────────────────────────┘
```

**Why it's necessary**:
- Hooks have strict timeout limits (&lt;2s typically)
- AI compression takes 10-90 seconds
- Worker architecture allows hooks to return immediately

**Implementation**:
```typescript
// Worker service (Express.js)
app.post('/api/compress', async (req, res) => {
  const { toolOutput, metadata } = req.body;

  // Acknowledge immediately
  res.status(202).json({ status: 'accepted', job_id: generateId() });

  // Process in background
  processCompression(toolOutput, metadata).catch(console.error);
});

async function processCompression(toolOutput: string, metadata: object) {
  // AI compression (expensive, takes time)
  const observation = await aiAgent.compress({
    input: toolOutput,
    context: metadata,
    model: 'claude-haiku-4-5'
  });

  // Save to both databases
  await Promise.all([
    chromaDB.insert({ id: observation.id, embedding: observation.embedding }),
    sqlite.insert('observations', observation)
  ]);
}
```

**Management**: Use PM2 (Node.js) or systemd (Linux) for worker lifecycle.

### Pattern 3: Progressive Disclosure Search

**When to use**: Applications needing efficient context retrieval.

```
┌─────────────────────────────────────────────────────────────┐
│ 1. Vector search → Top 5 candidates (index: ~50 tokens each)│
│ 2. Review titles → Identify 2 relevant items                 │
│ 3. Full retrieval → Complete details (~500 tokens each)      │
│ 4. Token savings → 10x compared to loading all              │
└─────────────────────────────────────────────────────────────┘
```

**API design**:
```typescript
// Step 1: Index format (cheap)
GET /api/observations?query=authentication&format=index&limit=5
Response: [
  { id: 1, title: "JWT auth bug fix", date: "2024-11-20", type: "bugfix" },
  { id: 2, title: "OAuth integration", date: "2024-11-19", type: "feature" },
  ...
]

// Step 2: User reviews titles and selects id=1

// Step 3: Full format (expensive)
GET /api/observations/1?format=full
Response: {
  id: 1,
  title: "JWT auth bug fix",
  content: "[Full 500-token observation with details...]",
  metadata: {...}
}
```

**Token efficiency**:
- Index format: 5 results × 50 tokens = 250 tokens
- Full format: 5 results × 500 tokens = 2,500 tokens
- Savings: 90% reduction by only loading relevant items

### Pattern 4: Real-Time Transcript Transformation

**When to use**: Applications hitting context limits mid-session.

```
┌─────────────────────────────────────────────────────────────┐
│ Original Transcript → [AI Compression] → Compressed          │
│                       (background)       Transcript           │
│                                       → Continue Session      │
└─────────────────────────────────────────────────────────────┘
```

**Endless Mode implementation**:
```typescript
// Hook intercepts tool output before returning to main thread
async function postToolUseHook(toolResult: ToolResult): Promise<ToolResult> {
  if (!ENDLESS_MODE_ENABLED) return toolResult;

  // Original output
  const originalOutput = toolResult.content; // 5,000 tokens

  // Compress with timeout
  const compressed = await Promise.race([
    compressObservation(originalOutput),
    timeout(90000, 'Compression timeout')
  ]);

  if (compressed) {
    // Replace original with compressed version
    toolResult.content = compressed.observation; // 1,000 tokens
    toolResult.metadata.compressed = true;
    toolResult.metadata.original_size = originalOutput.length;
  }

  return toolResult; // Returned to main transcript
}
```

**Benefits**:
- Enables indefinite session lengths
- Transparent to user (compression happens in background)
- Graceful fallback if compression times out

**Applicability**: Long research sessions, creative writing, extended consultations.

### Pattern 5: Dual-Database Complementarity

**When to use**: Always (fundamental to architecture).

```
┌─────────────────────────────────────────────────────────────┐
│ ChromaDB/Pinecone → Semantic similarity ranking              │
│                     ("what is this like?")                   │
│                                                              │
│ SQLite/PostgreSQL → Temporal ordering & metadata filtering   │
│                     ("when did this happen?")                │
└─────────────────────────────────────────────────────────────┘
```

**Schema design**:

```sql
-- SQL: Temporal ordering and metadata
CREATE TABLE observations (
  id INTEGER PRIMARY KEY,
  content TEXT NOT NULL,
  date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  type TEXT, -- bugfix, feature, decision, discovery
  project TEXT,
  session_id INTEGER,
  FOREIGN KEY (session_id) REFERENCES sessions(id)
);

CREATE INDEX idx_date ON observations(date DESC);
CREATE INDEX idx_project_date ON observations(project, date DESC);
```

```typescript
// Vector: Semantic embeddings
interface VectorRecord {
  id: string;
  embedding: number[]; // 1536-dimensional vector
  metadata: {
    date: string;
    type: string;
    project: string;
  }
}

// Insert to both databases
await Promise.all([
  chromaDB.add({
    id: observation.id,
    embedding: observation.embedding,
    metadata: { date, type, project }
  }),
  sqlite.run(
    'INSERT INTO observations (id, content, date, type, project) VALUES (?, ?, ?, ?, ?)',
    [observation.id, observation.content, date, type, project]
  )
]);
```

**Query patterns**:

```typescript
// Semantic search + temporal filter
async function searchWithTimeFilter(query: string, days: number) {
  // 1. Vector search (semantic ranking)
  const vectorResults = await chromaDB.query({
    query: query,
    limit: 20
  });

  // 2. SQL filter (temporal)
  const filteredIds = vectorResults
    .map(r => r.id)
    .filter(id => {
      const row = sqlite.get(
        'SELECT date FROM observations WHERE id = ? AND date > ?',
        [id, Date.now() - days * 24 * 60 * 60 * 1000]
      );
      return row !== undefined;
    });

  // 3. Return results (vector-ranked, time-filtered)
  return vectorResults.filter(r => filteredIds.includes(r.id));
}
```

**Why both are necessary**:
- Vector databases **cannot** efficiently sort by time (fundamental limitation)
- SQL databases **cannot** efficiently search by meaning
- Both are complementary, not redundant

---

## Implementation Considerations by Domain

| Domain | Compression Ratio | Key Challenge | Dual-DB Priority | Session Length |
|--------|------------------|---------------|------------------|---------------|
| Customer Support | 70-85% | Real-time response | Vector > SQL | Minutes-Hours |
| Legal Review | 80-90% | Precision & attribution | SQL > Vector | Days-Weeks |
| Research | 85-95% | Long-term coherence | Vector = SQL | Weeks-Months |
| Creative Writing | 75-85% | Narrative voice | Vector > SQL | Days-Months |
| Healthcare | 70-80% | Regulatory compliance | SQL > Vector | Years |
| Sales/BD | 75-85% | Relationship context | Vector > SQL | Weeks-Months |
| Code Review | 80-90% | Technical accuracy | SQL > Vector | Hours-Days |
| Education | 70-85% | Adaptive pacing | Vector = SQL | Months-Years |

**Compression ratio factors**:
- Technical content compresses better (90%) than conversational (70%)
- Structured data compresses better than unstructured
- Repetitive patterns compress better than novel exploration

**Database priority**:
- **Vector > SQL**: Semantic search more important (customer support, creative work)
- **SQL > Vector**: Temporal ordering critical (legal, healthcare, finance)
- **Vector = SQL**: Both equally important (research, education)

---

## Business Model Opportunities

### 1. Infrastructure Provider
**Strategy**: Build compression layer for specific AI platforms.

**Examples**:
- Claude-mem for Claude Code (current implementation)
- Cursor Memory Extension
- ChatGPT Enterprise Plugin
- GitHub Copilot Workspace Memory

**Revenue model**: Usage-based pricing on compression API calls.

**Go-to-market**:
- Open source core for adoption
- Enterprise features: multi-user, compliance, advanced search
- Per-seat pricing for teams

### 2. Platform Feature
**Strategy**: AI companies add this as a core product feature.

**Potential adopters**:
- Anthropic, OpenAI, Google, Mistral
- Competitive differentiator: "Our AI remembers across sessions"
- Premium tier feature for power users

**Value proposition**:
- 80% cost reduction at scale
- Enables longer-term customer relationships
- Competitive moat vs rivals without memory

### 3. Vertical SaaS
**Strategy**: Build domain-specific memory systems.

**Opportunities**:
- Healthcare: Patient history compression (HIPAA-compliant)
- Legal: Contract review memory (audit trails)
- Finance: Investment decision tracking (regulatory compliance)
- Sales: CRM with AI memory layer

**Revenue model**: Per-seat SaaS pricing ($50-200/user/month).

**Differentiation**: Domain-specific schemas, compliance, integrations.

### 4. Open Source Core + Enterprise
**Strategy**: Open source the compression architecture, monetize enterprise features.

**Open source**:
- Core compression algorithms
- Dual-database reference implementation
- Hook-based integration patterns
- Basic search functionality

**Enterprise**:
- Multi-user collaboration
- Role-based access control
- Advanced search with filters
- Compliance features (HIPAA, SOC 2)
- Self-hosted deployment options
- SLA and support

**Revenue model**: Enterprise licenses, hosting, support contracts.

---

## Next Steps for Productization

### Phase 1: Documentation & Reference Implementation
1. **Architecture Guide**: Create comprehensive developer documentation
2. **Reference SDK**: Python + TypeScript SDKs for easy integration
3. **Integration Examples**: Code samples for 3-5 platforms
4. **Performance Benchmarks**: Publish compression ratios, cost savings

**Timeline**: 4-6 weeks

### Phase 2: Case Studies & Pilots
1. **Healthcare pilot**: Partner with telemedicine platform
2. **Customer support pilot**: Integrate with Zendesk/Intercom
3. **Research pilot**: Deploy for academic literature review
4. **Metrics**: Measure compression ratios, cost savings, user satisfaction

**Timeline**: 2-3 months

### Phase 3: Platform Expansion
1. **Cursor integration**: Memory layer for Cursor IDE
2. **ChatGPT Enterprise plugin**: Memory for team workspaces
3. **VS Code extension**: Language model tool integration
4. **Partnerships**: Integrate with AI platforms

**Timeline**: 3-6 months

### Phase 4: Vertical SaaS Launch
1. **Pick vertical**: Healthcare or legal (high compliance requirements)
2. **Build domain schemas**: Specialized observation types
3. **Compliance certifications**: HIPAA, SOC 2
4. **Go-to-market**: Target specific use case (e.g., telemedicine)

**Timeline**: 6-12 months

---

## Key Insight: AI Infrastructure, Not a Tool

**The fundamental realization**: This is not a coding tool, it's **AI infrastructure**.

Any AI application that accumulates context over time needs this architecture:

```
┌─────────────────────────────────────────────────────────────┐
│ Input: Long transcripts being re-synthesized                │
│ Transform: 80% compression via AI summarization             │
│ Store: Dual-database (semantic + temporal)                  │
│ Retrieve: Progressive disclosure for token efficiency       │
│ Output: Indefinite session lengths with full memory         │
└─────────────────────────────────────────────────────────────┘
```

**Economic moat**: First mover in "AI episodic memory infrastructure" could become the standard layer between AI models and applications—similar to how databases sit between applications and storage.

**Market size**: Every AI application with long-running sessions needs this. That's:
- Customer support platforms (Zendesk, Intercom, Freshdesk)
- AI coding assistants (Cursor, GitHub Copilot, Replit)
- Healthcare AI (telemedicine, diagnostics)
- Legal tech (contract review, case research)
- Creative tools (writing, video editing, design)
- Research platforms (literature review, data analysis)

**Total addressable market**: Multi-billion dollar opportunity.

---

## Comparison: Claude-Mem vs Alternatives

| Feature | Claude-Mem | RAG | Context Caching | Threads API |
|---------|-----------|-----|----------------|-------------|
| **Semantic search** | ✅ (ChromaDB) | ✅ | ❌ | ❌ |
| **Temporal ordering** | ✅ (SQL) | ❌ | ❌ | ✅ |
| **Real-time compression** | ✅ (Endless Mode) | ❌ | ❌ | ❌ |
| **Episodic memory** | ✅ (AI's own actions) | ❌ (external docs) | ❌ | ✅ |
| **Cost reduction** | 80% | 0% (adds cost) | 50% | 0% |
| **Session length** | Indefinite | Limited | Limited | Limited |
| **Progressive disclosure** | ✅ (10x savings) | ❌ | ❌ | ❌ |
| **Dual-database** | ✅ (complementary) | Single | N/A | Single |

**Key differentiators**:
1. **Compression**: Claude-mem compresses, alternatives don't
2. **Dual-database**: Solves both semantic and temporal queries
3. **Episodic**: Captures AI's own actions, not external knowledge
4. **Cost**: 80% reduction vs adding cost (RAG) or no reduction (others)
5. **Indefinite sessions**: Endless Mode enables unlimited length

---

## Conclusion

Claude-mem's architecture represents a **general-purpose solution to AI context management** applicable across the entire AI industry.

The pattern is universal:
- ✅ 80% token compression
- ✅ Dual-database architecture
- ✅ Episodic memory capture
- ✅ Real-time transcript transformation
- ✅ Progressive disclosure retrieval

**Any AI application that accumulates context over time benefits from this architecture.**

The question is not *if* this pattern will become standard, but *who* will implement it first across different domains.

---

## Further Reading

<CardGroup cols={2}>
  <Card title="Architecture Overview" icon="sitemap" href="/architecture/overview">
    Detailed system architecture for claude-mem
  </Card>
  <Card title="Endless Mode" icon="infinity" href="/endless-mode/quickstart">
    Real-time compression for indefinite sessions
  </Card>
  <Card title="Dual-Database Design" icon="database" href="/architecture/search-architecture">
    Why vector + SQL databases are complementary
  </Card>
  <Card title="Progressive Disclosure" icon="magnifying-glass" href="/progressive-disclosure">
    Token-efficient context retrieval patterns
  </Card>
</CardGroup>

---

## Get Involved

Interested in applying this architecture to your domain?

- **GitHub**: [thedotmack/claude-mem](https://github.com/thedotmack/claude-mem)
- **Issues**: Share your use case or ask questions
- **Contribute**: Submit PRs for new integrations or patterns

**Let's build the future of AI memory infrastructure together.**
