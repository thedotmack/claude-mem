---
title: "OpenAI-Compatible Provider"
description: "Use any OpenAI-compatible API endpoint for observation extraction, including OpenRouter, CLIProxyAPI, local models, and more"
---

# OpenAI-Compatible Provider

Claude-mem supports any OpenAI-compatible chat completions API as an alternative provider for observation extraction. This includes [OpenRouter](https://openrouter.ai), [CLIProxyAPI](https://gist.github.com/chandika/c4b64c5b8f5e29f6112021d46c159fdd), local model servers, and any other endpoint that implements the OpenAI chat completions format.

<Tip>
**Flexible endpoints**: Point this provider at OpenRouter for 100+ models, CLIProxyAPI for Claude via your subscription, or any local model server with an OpenAI-compatible API.
</Tip>

## Why Use OpenAI-Compatible?

- **Endpoint flexibility**: Use OpenRouter, CLIProxyAPI, Ollama, LM Studio, or any OpenAI-compatible server
- **Access to 100+ models**: Via OpenRouter, choose from models across multiple providers
- **Free tier options**: Several high-quality models are completely free via OpenRouter
- **Cost flexibility**: Pay-as-you-go pricing on premium models with no commitments
- **Seamless fallback**: Automatically falls back to Claude if the API is unavailable
- **Hot-swappable**: Switch providers without restarting the worker
- **Multi-turn conversations**: Full conversation history maintained across API calls

## Default Endpoint

The default API endpoint is `https://openrouter.ai/api/v1/chat/completions` (OpenRouter). Override this with `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL` to use a different endpoint.

### Example: Using CLIProxyAPI

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY": "your-key",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL": "http://localhost:8317/v1/chat/completions",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL": "claude-sonnet-4-5-20250514"
}
```

## Local Models

Any server that exposes an OpenAI-compatible `/v1/chat/completions` endpoint works. Set `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL` to point at your local server.

### Ollama

[Ollama](https://ollama.com) serves local models with an OpenAI-compatible API on port 11434.

```bash
# Pull a model
ollama pull llama3.1:8b

# Ollama's OpenAI-compat endpoint is at /v1/chat/completions
```

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY": "ollama",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL": "http://localhost:11434/v1/chat/completions",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL": "llama3.1:8b"
}
```

<Note>
Ollama doesn't require a real API key, but the field can't be empty. Use any non-empty string like `"ollama"`.
</Note>

### LM Studio

[LM Studio](https://lmstudio.ai) runs local models with an OpenAI-compatible server on port 1234 by default.

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY": "lm-studio",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL": "http://localhost:1234/v1/chat/completions",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL": "your-loaded-model-name"
}
```

### CLIProxyAPI (Claude via subscription)

If you have a Claude Code Max subscription, you can use [CLIProxyAPI](https://gist.github.com/chandika/c4b64c5b8f5e29f6112021d46c159fdd) to route magic-claude-mem through your subscription. CLIProxyAPI is an open-source proxy that converts API key requests into OAuth-authenticated calls to Anthropic's API, handling token refresh automatically.

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY": "your-proxy-key",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL": "http://localhost:8317/v1/chat/completions",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL": "claude-sonnet-4-5-20250514"
}
```

### Other Compatible Servers

Any server implementing the OpenAI chat completions format works, including:
- **vLLM** — high-throughput inference server
- **text-generation-webui** (oobabooga) — with OpenAI extension enabled
- **LocalAI** — drop-in OpenAI replacement
- **llama.cpp server** — lightweight C++ inference

## Free Models on OpenRouter

OpenRouter actively supports democratizing AI access by offering free models. These are production-ready models suitable for observation extraction.

### Featured Free Models

| Model | ID | Parameters | Context | Best For |
|-------|------|------------|---------|----------|
| **Xiaomi MiMo-V2-Flash** | `xiaomi/mimo-v2-flash:free` | 309B (15B active, MoE) | 256K | Reasoning, coding, agents |
| **Gemini 2.0 Flash** | `google/gemini-2.0-flash-exp:free` | -- | 1M | General purpose |
| **Gemini 2.5 Flash** | `google/gemini-2.5-flash-preview:free` | -- | 1M | Latest capabilities |
| **DeepSeek R1** | `deepseek/deepseek-r1:free` | 671B | 64K | Reasoning, analysis |
| **Llama 3.1 70B** | `meta-llama/llama-3.1-70b-instruct:free` | 70B | 128K | General purpose |
| **Llama 3.1 8B** | `meta-llama/llama-3.1-8b-instruct:free` | 8B | 128K | Fast, lightweight |
| **Mistral Nemo** | `mistralai/mistral-nemo:free` | 12B | 128K | Efficient performance |

<Note>
**Default Model**: Claude-mem uses `xiaomi/mimo-v2-flash:free` by default -- a 309B parameter mixture-of-experts model that ranks #1 on SWE-bench Verified and excels at coding and reasoning tasks.
</Note>

### Free Model Considerations

- **Rate limits**: Free models may have stricter rate limits than paid models
- **Availability**: Free capacity depends on provider partnerships and demand
- **Queue times**: During peak usage, requests may be queued briefly
- **Max tokens**: Most free models support 65,536 completion tokens

All free models support:
- Tool use and function calling
- Temperature and sampling controls
- Stop sequences
- Streaming responses

## Getting an API Key

### For OpenRouter

1. Go to [OpenRouter](https://openrouter.ai)
2. Sign in with Google, GitHub, or email
3. Navigate to [API Keys](https://openrouter.ai/keys)
4. Click **Create Key**
5. Copy and securely store your API key

<Tip>
**Free to start**: No credit card required to create an account or use free models. Add credits only if you want to use premium models.
</Tip>

### For Other Endpoints

Check your endpoint provider's documentation for API key generation.

## Configuration

### Settings Overview

| Setting | Default | Description |
|---------|---------|-------------|
| `MAGIC_CLAUDE_MEM_PROVIDER` | `claude` | Set to `openai-compat` to use this provider |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY` | -- | API key for authentication |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL` | `xiaomi/mimo-v2-flash:free` | Model identifier to use |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL` | OpenRouter URL | API endpoint URL |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_CONTEXT_MESSAGES` | `20` | Conversation history limit |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_TOKENS` | `100000` | Token budget safety limit |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_SITE_URL` | -- | Analytics attribution URL |
| `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_APP_NAME` | `magic-claude-mem` | Analytics app name |

### Settings Reference

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY`

The API key sent in the `Authorization: Bearer` header. Where you get this depends on your endpoint:

- **OpenRouter**: Generate at [openrouter.ai/keys](https://openrouter.ai/keys)
- **CLIProxyAPI**: The proxy key configured during setup
- **Ollama / LM Studio**: These don't validate keys, but the field must be non-empty -- use any string like `"ollama"`

Can also be set via the `OPENAI_COMPAT_API_KEY` environment variable in `~/.magic-claude-mem/.env`. The settings file value takes precedence.

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL`

The model identifier passed in the API request body. Format depends on your endpoint:

- **OpenRouter**: `provider/model-name` format, e.g. `xiaomi/mimo-v2-flash:free`, `anthropic/claude-3.5-sonnet`
- **Ollama**: The model tag you pulled, e.g. `llama3.1:8b`, `codestral:latest`
- **LM Studio**: The model name shown in the UI
- **CLIProxyAPI**: The full Anthropic model ID, e.g. `claude-sonnet-4-5-20250514`

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_BASE_URL`

The full URL of the chat completions endpoint. When empty, defaults to `https://openrouter.ai/api/v1/chat/completions`.

Common values:

| Endpoint | URL |
|----------|-----|
| OpenRouter (default) | `https://openrouter.ai/api/v1/chat/completions` |
| Ollama | `http://localhost:11434/v1/chat/completions` |
| LM Studio | `http://localhost:1234/v1/chat/completions` |
| CLIProxyAPI | `http://localhost:8317/v1/chat/completions` |
| vLLM | `http://localhost:8000/v1/chat/completions` |

<Warning>
This must be the full URL including the path (e.g. `/v1/chat/completions`), not just the host.
</Warning>

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_CONTEXT_MESSAGES`

Maximum number of messages retained in the conversation history sent to the API. Older messages are dropped when this limit is exceeded. Range: 1--100.

- **Lower values** (5--10): Less context per request, lower cost, faster responses. Good for expensive models or endpoints with small context windows.
- **Higher values** (30--50): More context, better coherence across a long session. Good for models with large context windows.
- **Default (20)**: Balanced for most use cases.

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_TOKENS`

Estimated token budget for the conversation history. The agent estimates tokens at 4 characters per token and drops old messages when the estimate exceeds this limit. Range: 1,000--1,000,000.

This is a safety net to prevent sending oversized requests to endpoints with limited context windows. The default of 100,000 is conservative for most models.

- **Local models with 4K--8K context**: Set to `4000`--`8000`
- **Models with 32K context**: Set to `30000`
- **Models with 128K+ context**: The default `100000` is fine

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_SITE_URL`

Sent as the `HTTP-Referer` header. Used by OpenRouter for analytics attribution -- tells them which site is generating traffic. Ignored by most other endpoints. Leave empty if not using OpenRouter.

#### `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_APP_NAME`

Sent as the `X-Title` header. Used by OpenRouter to display your app name in their dashboard. Defaults to `magic-claude-mem`. Ignored by most other endpoints.

### Using the Settings UI

1. Open the viewer at http://localhost:37777
2. Click the **gear icon** to open Settings
3. Under **AI Provider**, select **OpenAI-Compatible (multi-model)**
4. Enter your API key
5. Optionally select a different model

Settings are applied immediately -- no restart required.

### Manual Configuration

Edit `~/.magic-claude-mem/settings.json`:

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY": "sk-or-v1-your-key-here",
  "MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MODEL": "xiaomi/mimo-v2-flash:free"
}
```

Alternatively, set the API key via environment variable:

```bash
export OPENAI_COMPAT_API_KEY="sk-or-v1-your-key-here"
```

The settings file takes precedence over the environment variable.

## Migration from OpenRouter Settings

If you previously used `MAGIC_CLAUDE_MEM_OPENROUTER_*` settings, they are automatically migrated to `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_*` on the next worker startup. No manual action required.

## Model Selection Guide

### For Free Usage (No Cost)

**Recommended**: `xiaomi/mimo-v2-flash:free`
- Best-in-class performance on coding benchmarks
- 256K context window handles large observations
- 65K max completion tokens
- Mixture-of-experts architecture (15B active parameters)

**Alternatives**:
- `google/gemini-2.0-flash-exp:free` - 1M context, Google's flagship
- `deepseek/deepseek-r1:free` - Excellent reasoning capabilities
- `meta-llama/llama-3.1-70b-instruct:free` - Strong general purpose

### For Paid Usage (Higher Quality/Speed)

| Model | Price (per 1M tokens) | Best For |
|-------|----------------------|----------|
| `anthropic/claude-3.5-sonnet` | $3 in / $15 out | Highest quality observations |
| `google/gemini-2.0-flash` | $0.075 in / $0.30 out | Fast, cost-effective |
| `openai/gpt-4o` | $2.50 in / $10 out | GPT-4 quality |

## Context Window Management

The OpenAI-compatible agent implements intelligent context management to prevent runaway costs:

### Automatic Truncation

The agent uses a sliding window strategy:
1. Checks if message count exceeds `MAX_CONTEXT_MESSAGES` (default: 20)
2. Checks if estimated tokens exceed `MAX_TOKENS` (default: 100,000)
3. If limits exceeded, keeps most recent messages only
4. Logs warnings with dropped message counts

### Token Estimation

- Conservative estimate: 1 token = 4 characters
- Used for proactive context management
- Actual usage logged from API response

### Cost Tracking

Logs include detailed usage information:

```
OpenAI-compat API usage: {
  model: "xiaomi/mimo-v2-flash:free",
  inputTokens: 2500,
  outputTokens: 1200,
  totalTokens: 3700,
  estimatedCostUSD: "0.00",
  messagesInContext: 8
}
```

## Provider Switching

You can switch between providers at any time:

- **No restart required**: Changes take effect on the next observation
- **Conversation history preserved**: When switching mid-session, the new provider sees the full conversation context
- **Seamless transition**: All providers use the same observation format

### Switching via UI

1. Open Settings in the viewer
2. Change the **AI Provider** dropdown
3. The next observation will use the new provider

### Switching via Settings File

```json
{
  "MAGIC_CLAUDE_MEM_PROVIDER": "openai-compat"
}
```

## Fallback Behavior

If the OpenAI-compatible API encounters errors, magic-claude-mem automatically falls back to the Claude Agent SDK:

**Triggers fallback:**
- Rate limiting (HTTP 429)
- Server errors (HTTP 500, 502, 503)
- Network issues (connection refused, timeout)
- Generic fetch failures

**Does not trigger fallback:**
- Missing API key (logs warning, uses Claude from start)
- Invalid API key (fails with error)

When fallback occurs:
1. A warning is logged
2. Any in-progress messages are reset to pending
3. Claude SDK takes over with the full conversation context

<Note>
**Fallback is transparent**: Your observations continue processing without interruption. The fallback preserves all conversation context.
</Note>

## Multi-Turn Conversation Support

The OpenAI-compatible agent maintains full conversation history across API calls:

```
Session Created
  |
Load Pending Messages (observations from queue)
  |
For each message:
  -> Add to conversation history
  -> Call API with FULL history
  -> Parse XML response
  -> Store observations in database
  -> Sync to Chroma vector DB
  |
Session complete
```

This enables:
- Coherent multi-turn exchanges
- Context preservation across observations
- Seamless provider switching mid-session

## Troubleshooting

### "OpenAI-compatible API key not configured"

Either:
- Set `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_API_KEY` in `~/.magic-claude-mem/settings.json`, or
- Set the `OPENAI_COMPAT_API_KEY` environment variable

### Rate Limiting

Free models may have rate limits during peak usage. If you hit rate limits:
- Claude-mem automatically falls back to Claude SDK
- Consider switching to a different free model
- Add credits for premium model access

### Model Not Found

Verify the model ID is correct:
- Check [OpenRouter Models](https://openrouter.ai/models) for current availability
- Use the `:free` suffix for free model variants
- Model IDs are case-sensitive

### High Token Usage Warning

If you see warnings about high token usage (>50,000 per request):
- Reduce `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_CONTEXT_MESSAGES`
- Reduce `MAGIC_CLAUDE_MEM_OPENAI_COMPAT_MAX_TOKENS`
- Consider a model with larger context window

### Connection Errors

If you see connection errors:
- Check your internet connection
- Verify the API endpoint is accessible
- The agent will automatically fall back to Claude

## API Details

The provider uses a standard OpenAI-compatible REST API:

**Default Endpoint**: `https://openrouter.ai/api/v1/chat/completions`

**Headers**:
```
Authorization: Bearer {apiKey}
HTTP-Referer: https://github.com/doublefx/magic-claude-mem
X-Title: magic-claude-mem
Content-Type: application/json
```

**Request Format**:
```json
{
  "model": "xiaomi/mimo-v2-flash:free",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."}
  ],
  "temperature": 0.3,
  "max_tokens": 4096
}
```

## Comparing Providers

| Feature | Claude (SDK) | Gemini | OpenAI-Compatible |
|---------|-------------|--------|------------|
| **Cost** | Pay per token | Free tier + paid | Free models + paid |
| **Models** | Claude only | Gemini only | 100+ models |
| **Quality** | Highest | High | Varies by model |
| **Rate limits** | Based on tier | 5-4000 RPM | Varies by model |
| **Fallback** | N/A (primary) | -> Claude | -> Claude |
| **Setup** | Automatic | API key required | API key required |
| **Custom endpoint** | No | No | Yes |

<Tip>
**Recommendation**: Start with the free `xiaomi/mimo-v2-flash:free` model for zero-cost observation extraction. If you need higher quality or encounter rate limits, switch to Claude or add credits for premium models.
</Tip>

## Next Steps

- [Configuration](/configuration) - Full settings reference
- [Gemini Provider](/usage/gemini-provider) - Alternative free provider
- [Getting Started](/usage/getting-started) - Basic usage guide
- [Troubleshooting](/troubleshooting) - Common issues
